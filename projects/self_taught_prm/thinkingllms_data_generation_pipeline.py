# -*- coding: utf-8 -*-
"""ThinkingLLMs - Data generation pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14HUMY_eHTA-tpEFO62kMN9XH_DDdnwWU
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install "distilabel[hf-inference-endpoints,hf-transformers]" pynvml bitsandbytes -qqq

import os
from typing import Dict, Any, Union

from datasets import load_dataset

from distilabel.pipeline import Pipeline
from distilabel.llms.huggingface import InferenceEndpointsLLM
from distilabel.steps.tasks import Task
from distilabel.steps import RewardModelScore
from distilabel.steps.tasks.typing import ChatType
from distilabel.steps.typing import StepColumns

from prompt import THINKING_LLM_MATH_PROMPT
from grading.grader import grade_answer
from parser import extract_result_from_boxed


hf_token = os.getenv('HF_TOKEN')

MODEL_ID = "meta-llama/Llama-3.1-8B-Instruct"

llm = InferenceEndpointsLLM(
    model_id=MODEL_ID,
    tokenizer_id=MODEL_ID,
    generation_kwargs={
        "temperature": 0.8,
        "max_new_tokens": 2048,
        "top_p": 0.95,
    },
    api_key=hf_token
)

class ThinkingLLMGeneration(Task):
    @property
    def inputs(self) -> StepColumns:
        return ["problem"]

    def format_input(self, input: Dict[str, Any]) -> ChatType:
        return [
            # {"role": "user", "content": LLAMA3_QUERY_TEMPLATE.render(Question=input["problem"])}
            {"role": "user", "content": THINKING_LLM_MATH_PROMPT.format(instruction=input["problem"])}
        ]

    @property
    def outputs(self) -> StepColumns:
        return ["thought", "output", "parsed_output", "correctness"]

    def format_output(
        self, output: Union[str, None], input: Union[Dict[str, Any], None] = None
    ) -> Dict[str, Any]:
        if output is None:
            return {"thought": None, "output": None, "parsed_output": None, "correctness": None}

        # Split the output into thought and final response
        parts = output.split("<R>")

        if len(parts) < 2:
            return {"thought": output.strip(), "output": None, "parsed_output": None, "correctness": None}

        thought = parts[0].strip()
        final_output = "<R>".join(parts[1:]).strip()

        parsed_output = extract_result_from_boxed(final_output)
        correctness = grade_answer(parsed_output, input["answer"])

        return {"thought": thought, "output": final_output, "parsed_output": parsed_output, "correctness": correctness}

DEBUG_SIZE = 10
dataset = (
    load_dataset(
        "yentinglin/MATH-prm800k",
        split="test"
    )
    .shuffle(seed=42)
    .select(range(DEBUG_SIZE))
)

with Pipeline(name="thinking-llms") as pipeline:
    generation_with_thoughts = ThinkingLLMGeneration(
        llm=llm,
        num_generations=2, # paper generates 8 samples, I use 2 for demo purposes
    )
    # judge_responses = RewardModelScore(
    #     model="SteveTran/ArmoRM-Llama3-8B-v0.1-4bit",
    #     device_map="auto",
    #     trust_remote_code=True,
    #     input_mappings={"response": "output"},
    #     torch_dtype="bfloat16"
    # )
    # generation_with_thoughts >> judge_responses

if __name__ == "__main__":
    distiset = pipeline.run(dataset=dataset, use_cache=False)
    distiset.push_to_hub(
        "yentinglin/test_pipeline",
        include_script=True,
        private=True
    )

